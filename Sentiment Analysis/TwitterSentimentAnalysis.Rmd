---
title: "Twitter Sentiment Analysis"
author: "Dhrubasattwata Roy Choudhury"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Using publicly available R packages that interact with major platforms such as RFacebook (Facebook), rtweet (Twitter) and Rblpapi (Bloomberg), RStudio’s visual interface and plotting tools have eased the way for sentiment analysis.

### Import the required packages

```{r}
library("rtweet")
library("dplyr")
library("tidyr")
library("tidyverse")
library("httpuv")
library("tidytext")
library("textdata")
```

### You need a twitter developer account
Visit https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html for details.

### Store api keys (this will not be displayed for obvious reasons)

Create the five variables: app_name, api_key, api_secret_key,access_token, access_token_secret
```{r, include=FALSE}
app_name <- "RSentimentAnalysis_dsrc"
api_key <- "tUcVGVKyxp8SSZKJBIyJdtgmO"
api_secret_key <- "I1RBph9XkwJDHYHENov5vtICtVC61UfKWcPIpUAuxY6m03Jxbp"
access_token <- "304794066-oycFcljDl6VX8meHnqusJM3v8NllggXzWQGTNoHI"
access_token_secret <- "bOJwUDIKTgSMOzG1LGQXESZQoODhzgXnylpdwH6XD82Me"
```

### Authenticate via web browser
```{r}
token <- create_token(
  app = app_name,
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

get_token()

```

### Accessing Tweets

Only accessing English Tweets, because my language skills is limited.

```{r}
tweets1 <- search_tweets("#HagiaSophia", n=10000,include_rts = FALSE, lang="en")
tweets2 <- search_tweets("#Hagia_Sophia", n=10000,include_rts = FALSE, lang="en")
tweets3 <- search_tweets("#AyaSophia", n=10000,include_rts = FALSE, lang="en")
tweets4 <- search_tweets("#Aya_Sophia", n=10000,include_rts = FALSE, lang="en")
tweets <- rbind(tweets1, tweets2, tweets3, tweets4)
```

### Processing the Data
1. Keeping only the name and tweet  
2. Use pre-processing text transformations to clean up the tweets; this includes stemming words. An example of stemming is rolling the words “computer”, “computational” and “computation” to the root “comput”.  
3. Additional pre-processing involves converting all words to lower-case, removing links to web pages (http elements), and deleting punctuation as well as stop words. The tidytext package contains a list of over 1,000 stop words in the English language that are not helpful in determining the overall sentiment of a text body; these are words such as “I”, “ myself”, “ themselves”, “being” and “have”. We are using the tidytext package with an anti-join to remove the stop words from the tweets.

```{r}
# Keeping only the name and tweet
tweets.all<- tweets %>% select(screen_name,text)

## Clean up tweets
tweets.all$stripped_text1 <- gsub("http\\S+","",tweets.all$text)

# Use the unnest_tokens() function to convert to lowercase, remove punctuation and add ID
tweets.all_stem <- tweets.all %>% select(stripped_text1) %>% tidytext::unnest_tokens(word,stripped_text1)

# Remove Stop Words
cleaned_tweets.all <- tweets.all_stem %>% anti_join(stop_words)

## Let us also remove words like hagiasophia, ayasophia, sophia, hagia_sophia because this will not add any value to the analysis
x <- c("hagiasophia", "ayasophia", "sophia", "hagia_sophia", "ayasofya", "hagiasofia")
remove_words <- stop_words[1:6,]
remove_words[,1] <- x

cleaned_tweets.all <- cleaned_tweets.all %>% anti_join(remove_words) 
```

### Some Basic Analysis

Find the top 20 commonly used words in the set of tweets; this will give an overall picture of what the populations are most concerned about, and the extent to which they are engaged on these topics.

```{r}
cleaned_tweets.all %>%
  count(word,sort=TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(x=word,y=n)) + geom_col() + coord_flip() +theme_classic() +
  labs(x="Count",y="Unique Words", title= "Unique Words in tweets")
```

### Sentiment Analysis

Perform sentiment analysis using the Bing lexicon and get_sentiments function from the tidytext package. There are many libraries, dictionaries and packages available in R to evaluate the emotion prevalent in a text. The tidytext and textdata packages have such word-to-emotion evaluation repositories. Three of the general purpose lexicons are Bing, AFINN and nrc (from the textdata package).

```{r}
## Getting Sentiments
tidytext::get_sentiments("bing") %>% filter(sentiment=="positive")
tidytext::get_sentiments("bing") %>% filter(sentiment=="negative")
```

In contrast to Bing, the AFINN lexicon assigns a “positive” or “negative” score to each word in its lexicon; further sentiment analysis will then add up the emotion score to determine overall expression. A score greater than zero indicates positive sentiment, while a score less than zero would mean negative overall emotion. A calculated score of zero indicates neutral sentiment (neither positive or negative). We will do our analysis using Bing sentiments.

```{r}
bing_all <- cleaned_tweets.all %>%
  inner_join(get_sentiments("bing")) %>%
  count(word,sentiment,sort=TRUE) %>%
  ungroup()

bing_all %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word,n,fill=sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment,scales="free_y")+
  labs(title="Tweets for HagiaSophia",y="Contribution to Sentiment",x=NULL)+
  coord_flip() + theme_bw()
```

### Sentiment Score for each tweet

```{r message=FALSE}
sentiment_bing <- function(twt){
  #Step 1: perform basic cleaning of tweet
  twt_tbl <- tibble(text=twt) %>%
    mutate(stripped_text = gsub("http\\S+","",text)) %>%
    unnest_tokens(word,stripped_text) %>%
    anti_join(stop_words) %>% #remove stop_words
    inner_join(get_sentiments("bing")) %>% #merge with bing sentiments
    count(word,sentiment,sort=TRUE)%>%
    ungroup() %>%
    mutate( # Calculate Score : 1 for positive word, -1 for negatives
      score= case_when(
        sentiment == "negative" ~ n*(-1),
        sentiment == "postive" ~ n*1
      )
    ) #Calculate Total Score
  sent.score= case_when(
    nrow(twt_tbl) == 0~0,
    nrow(twt_tbl) >0~sum(twt_tbl$score)
  )
  zero.type = case_when(
    nrow(twt_tbl) == 0~"Type 1", #no words
    nrow(twt_tbl) >0~"Type 2" #sum of words is 0
  )
  list(score=sent.score,type=zero.type,twt_tbl=twt_tbl)
}

all_sentiment <- lapply(tweets$text,function(x){sentiment_bing(x)})
# all_sentiment
```

### Plotting the histogram of scores

```{r}
## Plotting Histogram

sentiment <- bind_rows(
  tibble(
    about = "#Hagia Sophia",
    score = unlist(map(all_sentiment,'score')),
    type = unlist(map(all_sentiment,'type'))
  )
)

ggplot(sentiment, aes(x=score)) + geom_histogram(bins=15, alpha = 1, color = "midnightblue", fill = "darkolivegreen") + ggtitle("Score of the tweets about Hagia Sophia") +  theme_bw()
```

As we can see, the twitter community which tweets in English did not find the recent conversion of Hagia Sophia into a mosque to be a positive change. 



 